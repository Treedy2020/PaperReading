{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分析transformer模型的参数量、计算量、中间激活、KV cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layer Norm和Batch Norm都是常用的归一化方法，它们的公式和常见放置位置如下：\n",
    "\n",
    "Batch Norm的公式为：\n",
    "$$\\hat{x}^{(k)}=\\frac{x^{(k)}-\\mathrm{E}[x^{(k)}]}{\\sqrt{\\mathrm{Var}[x^{(k)}]+\\epsilon}}$$\n",
    "$$y^{(k)}=\\gamma^{(k)}\\hat{x}^{(k)}+\\beta^{(k)}$$\n",
    "\n",
    "其中，$x^{(k)}$表示第$k$个特征维度的输入，$\\mathrm{E}[x^{(k)}]$和$\\mathrm{Var}[x^{(k)}]$分别表示在batch维度上第$k$个特征的均值和方差，$\\gamma^{(k)}$和$\\beta^{(k)}$是可学习的缩放和偏移参数。[1][4][5]\n",
    "\n",
    "Batch Norm常放置在卷积层或全连接层之后，激活函数之前。在CNN中，Batch Norm是在每个特征维度上归一化，对batch中HxW个元素求均值和方差。[1][4][8]\n",
    "\n",
    "Layer Norm的公式为：\n",
    "$$\\hat{x}^{(i)}=\\frac{x^{(i)}-\\mathrm{E}[x^{(i)}]}{\\sqrt{\\mathrm{Var}[x^{(i)}]+\\epsilon}}$$\n",
    "$$y^{(i)}=\\gamma\\hat{x}^{(i)}+\\beta$$\n",
    "\n",
    "其中，$x^{(i)}$表示第$i$个样本的所有特征，$\\mathrm{E}[x^{(i)}]$和$\\mathrm{Var}[x^{(i)}]$表示第$i$个样本特征的均值和方差，$\\gamma$和$\\beta$是可学习的缩放和偏移参数，对所有样本共享。[1][4][5]\n",
    "\n",
    "Layer Norm常用于RNN、Transformer等处理序列数据的模型中，它在每个样本上对所有特征做归一化。在Transformer中，Layer Norm常放置在每个子层（Self-Attention、Feed Forward）之后。[1][3][5]\n",
    "\n",
    "综上，Batch Norm和Layer Norm的主要区别在于：\n",
    "1. Batch Norm在batch维度归一化，Layer Norm在特征维度归一化  \n",
    "2. Batch Norm的缩放偏移参数$\\gamma$和$\\beta$与特征维度$k$相关，Layer Norm的$\\gamma$和$\\beta$对所有特征共享\n",
    "3. Batch Norm常用于CNN，Layer Norm常用于RNN和Transformer\n",
    "\n",
    "Layer Norm不依赖batch大小，适合变长序列的归一化，因此在NLP任务中应用更广泛。\n",
    "\n",
    "Citations:\n",
    "[1] https://www.cnblogs.com/shine-lee/p/11989612.html\n",
    "[2] https://qkunai.com/2023/08/28/%E6%89%8B%E6%8A%8A%E6%89%8B%E5%B8%A6%E4%BD%A0%E4%BA%86%E8%A7%A3%E5%8F%B1%E8%AF%A7%E9%A3%8E%E4%BA%91%E7%9A%84-transformer%E6%9E%B6%E6%9E%84%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90%E4%BB%A3%E7%A0%81/\n",
    "[3] https://wqw547243068.github.io/chatgpt_mimic\n",
    "[4] https://www.cnblogs.com/tian777/p/17911800.html\n",
    "[5] https://www.jiqizhixin.com/articles/2019-07-09-5\n",
    "[6] https://blog.csdn.net/qq_36560894/article/details/115017087\n",
    "[7] https://blog.csdn.net/Flying_sfeng/article/details/116540763\n",
    "[8] https://blog.csdn.net/m0_37192554/article/details/85049433\n",
    "[9] https://www.cnblogs.com/LittleHann/p/17318509.html\n",
    "[10] https://github.com/DA-southampton/NLP_ability/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/NLP%E4%BB%BB%E5%8A%A1%E4%B8%AD-layer-norm%E6%AF%94BatchNorm%E5%A5%BD%E5%9C%A8%E5%93%AA%E9%87%8C.md\n",
    "[11] https://www.zhihu.com/question/487766088/answer/2422936310\n",
    "[12] https://arxivtools.blob.core.windows.net/xueshuxiangzipaperhtml/2024_3_1/2402.19231.pdf\n",
    "[13] https://wap.sciencenet.cn/blog-3396477-1407467.html?mobile=1\n",
    "[14] https://blog.csdn.net/qq1145520074/article/details/79151948\n",
    "[15] https://www.cvmart.net/community/detail/1569\n",
    "[16] https://zh-v2.d2l.ai/d2l-zh-pytorch.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简单单层参数量分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 权重矩阵 $W^{Q}$、$W^{K}$、$W^{V}$  \n",
    "shape: (h, h)  \n",
    "参数量： $4*h^{2} + 4*h$  \n",
    "2. MLP部分  \n",
    "常规为4倍的前馈网络，两层的参数量均为 $4*h^{2}$，偏置为前层的$4h$ 以及后层的$h$，一共$8h^{2} + 5h$\n",
    "3. Layer Norm\n",
    "在Multihead Attention和FFN之后，各有一个，参数量为缩放因子和偏移因子，形状均为$h$，总大小 $4h$\n",
    "\n",
    "每一个Transformer模块的参数量共： $12h^{2} + 13h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单层计算量分析\n",
    "输入映射，不考虑位编码和词表映射的计算量，输入shape (batch_size, seq_len, hidden_size) -> (b, s, h)  \n",
    "考虑乘法和加法的数量级\n",
    "\n",
    "1.1 Q、K、V映射 \n",
    "\n",
    "(b, s, h) X (h, h) (b, s, h) -> (b, s, h) $2bsh^{2}*3$\n",
    "\n",
    "1.2 Multihead attention score 的计算 \n",
    "\n",
    "$Q\\times K^{T}$ (b, s, h)x(b, h, s) -> (b, s, s) $2bs^{2}h$\n",
    "\n",
    "1.3 计算 Value \n",
    "\n",
    "$\\rm{Softmax(score)} \\times V$ (b, s, s)x(b, s, h) -> (b, s, h) $2bs^{2}h$\n",
    "\n",
    "1.4 线性映射(本质是让Transformer知道应该关注Multihead的哪些部分)\n",
    "```\n",
    "在Multi-head Attention的实现中，在计算完attention score与Value的乘积后，还需要经过一次线性映射，主要有以下几个原因：\n",
    "\n",
    "1. 将多头attention的输出维度还原回与输入维度一致。由于在Multi-head Attention中，输入被拆分成多个头(head)分别计算attention，每个头的维度是原始维度除以头数。当把多个头的结果concat起来后，维度扩大到了原始维度的倍数。因此需要一个线性变换层将维度降回与输入一致，以便后续的残差连接和Layer Normalization操作。[1][4][5][8]\n",
    "\n",
    "2. 让模型学习不同头的attention结果的重要性。通过线性变换层，可以自动学习到不同头输出的重要程度，相当于给每个头的结果都乘以一个权重，再求和。这样可以让模型根据任务自适应地调整每个头的权重。[4][5]\n",
    "\n",
    "3. 提高模型的表达能力。这个线性变换实际上扮演了融合多头信息的角色，它可以捕捉不同子空间之间的相关性，从而得到一个信息更加丰富的向量表示。类似于CNN中使用1x1卷积来融合不同特征图的信息。[1][5]\n",
    "\n",
    "4. 防止attention结果过度拟合到某一个头上。如果没有这一步线性变换，模型可能会过度依赖某个头的结果，从而影响泛化性能。加入线性变换层相当于在多个头之间做了一个平滑。[4]\n",
    "\n",
    "综上，Multi-head Attention后的线性变换是非常必要和关键的一步，它让多头attention的结果维度与输入一致，并融合了不同头的信息，提高了模型的表达能力，让attention机制更加健壮。这也体现了Transformer设计的精巧之处。\n",
    "\n",
    "Citations:\n",
    "[1] https://www.cnblogs.com/xianbin7/p/11349312.html\n",
    "[2] https://www.zhihu.com/question/592626839\n",
    "[3] https://bbs.huaweicloud.com/blogs/382395\n",
    "[4] https://jimmy-walker.gitbook.io/tensorflow/attention-and-transformer\n",
    "[5] https://blog.csdn.net/qq_42363032/article/details/125988557\n",
    "[6] https://developer.aliyun.com/article/1462204\n",
    "[7] http://www.c-s-a.org.cn/csa/article/html/6944\n",
    "[8] https://blog.csdn.net/None_Pan/article/details/106414724\n",
    "```\n",
    "\n",
    "(b, s, h)x(h, h) -> $2bsh^{2}$\n",
    "\n",
    "1.5 FFN / MLP 网络结构\n",
    "\n",
    "(b, s, h)x(h, 4h) 和 (b, s, 4h)x(4h, s) -> $16bsh^{2}$\n",
    "\n",
    "故一个Transformer层的计算量大约是 $24bsh^{2} + 4bs^{2}h$\n",
    "\n",
    "1.6 `Logits`输出映射\n",
    "\n",
    "(b, s, h)x(h, V) -> $2bshV$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中间激活的显存占用（Bytes）\n",
    "这里主要考虑的是前向传递中计算得到，并且在后向传递过程中所需要的张量。前向和后向计算中，视参数使用2bytes的浮点数进行保存。注意mask矩阵的每个元素仅为 1 byte。\n",
    "\n",
    "1. 输入 x \n",
    "\n",
    "(b, s, h) -> $2bsh$ \n",
    "\n",
    "2. Multihead Attention 中的 Q / K / V\n",
    "\n",
    "(b, s, h)*3 -> $6bsh$\n",
    "\n",
    "3. 计算`Score`时需要保留 $Q\\times K^{T}$的中间结果和最终在V上的attention\n",
    "\n",
    "(b, head_num, s, h_per_head)x(b, head_num, h_per_head, s) -> (b, head_num, s, s) -> $2bs^{2} head num$*2\n",
    "\n",
    "4. droup_out保留droupout mask\n",
    "$bs^{2}head_num$\n",
    "\n",
    "4. 输出映射保留其输入以及droupout 的mask\n",
    "\n",
    "输入 (b, s, h) -> $2bsh$ / dropout (b, s, h) -> $bsh$\n",
    "\n",
    "5. MLP的\n",
    "(b, s, h) 第一个线性层输入/ (b, s, 4h) 激活函数输入 / (b, s, 4h)第二个线性层输入 / (b, s, h) / dropout mask \n",
    "$19bsh$\n",
    "\n",
    "6. Layer Norm\n",
    "4*(b, s, h) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现Llama Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    def __init__(self, n_head: int,\n",
    "                       hidden_dim: int,\n",
    "                       bias: bool= False):\n",
    "        super().__init__()\n",
    "        assert hidden_dim%n_head == 0, \"n_head must can be devided by hidden_dim.\"\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_dim = hidden_dim//n_head\n",
    "        self.n_head = n_head\n",
    "        \n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n",
    "        self.o_proj = nn.Linear(hidden_dim, hidden_dim, bias=bias)\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_states: torch.Tensor,\n",
    "                past_kv_states: Optional[Tuple[torch.Tensor]] = None,\n",
    "                attention_mask: Optional[torch.Tensor] = None,\n",
    "                use_cache: bool = False,\n",
    "                output_attentions: bool = False\n",
    "                ):\n",
    "        bsz, seq_len, _ = hidden_states.shape\n",
    "        \n",
    "        q = self.q_proj(hidden_states).view(bsz, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k =  self.k_proj(hidden_states).view(bsz, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = self.v_proj(hidden_states).view(bsz, seq_len, self.n_head, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        if past_kv_states is not None:\n",
    "            k, v  = torch.cat([past_kv_states[0], k], dim=2), torch.cat([past_kv_states[1], v], dim=2)        \n",
    "        past_kv_states = [k, v] if use_cache else None\n",
    "\n",
    "        attentions = torch.matmul(q, k.transpose(2, 3))/math.sqrt(self.head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attentions  = attentions + attention_mask\n",
    "            attention_dtype_min = torch.Tensor(torch.finfo(attentions.dtype).min, device=attentions.device, dtype=attentions.dtype)\n",
    "            attentions = torch.max(attentions, attention_dtype_min)\n",
    "            print(attentions.shape)\n",
    "        \n",
    "        attentions_out = torch.matmul(nn.functional.softmax(attentions, dim=-1, dtype=torch.float32), v)\n",
    "        attentions_out = attentions_out.transpose(1, 2).reshape(bsz, seq_len, self.hidden_dim)\n",
    "        attentions_out = self.o_proj(attentions_out)\n",
    "        \n",
    "        if not output_attentions:\n",
    "            attentions = None\n",
    "        return attentions_out, attentions,  past_kv_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 前馈中，先进行seq_len维度和n_head维度的transpose;\n",
    "2. attention_weights 在进行softmax操作时需要使用torch.float32作为结果的type，最终维度为-1；\n",
    "3. use_cache用于表征最后是否保留past_kv_states，而attention_output则用于表征最后是否保留attention的weights;\n",
    "4. 如果需要做dropout，则应该在计算attention_weights之后；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现Dot Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient implementation equivalent to the following:\n",
    "def scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None) -> torch.Tensor:\n",
    "    L, S = query.size(-2), key.size(-2)\n",
    "    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n",
    "    attn_bias = torch.zeros(L, S, dtype=query.dtype)\n",
    "    if is_causal:\n",
    "        assert attn_mask is None\n",
    "        temp_mask = torch.ones(L, S, dtype=torch.bool).tril(diagonal=0)\n",
    "        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n",
    "        attn_bias.to(query.dtype)\n",
    "\n",
    "    if attn_mask is not None:\n",
    "        if attn_mask.dtype == torch.bool:\n",
    "            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n",
    "        else:\n",
    "            attn_bias += attn_mask\n",
    "    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n",
    "    attn_weight += attn_bias\n",
    "    attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n",
    "    return attn_weight @ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1652, -0.0233,  0.1016,  ...,  0.0624,  0.0478,  0.1157],\n",
       "          [ 0.1713, -0.0242,  0.0906,  ...,  0.0377, -0.0134,  0.0919],\n",
       "          [ 0.1244, -0.0675,  0.1229,  ...,  0.0393,  0.0030,  0.0779],\n",
       "          ...,\n",
       "          [ 0.1832, -0.0359,  0.1621,  ...,  0.0104, -0.0540,  0.1037],\n",
       "          [ 0.1441, -0.0357,  0.1385,  ...,  0.0318, -0.0034,  0.1368],\n",
       "          [ 0.1330, -0.0050,  0.1480,  ...,  0.0344, -0.0021,  0.0700]],\n",
       " \n",
       "         [[ 0.3329, -0.1125,  0.1560,  ..., -0.0185, -0.0519,  0.2437],\n",
       "          [ 0.1985, -0.0326,  0.1246,  ..., -0.0531, -0.0262,  0.2078],\n",
       "          [ 0.1980, -0.0452,  0.0998,  ..., -0.0817, -0.0298,  0.1881],\n",
       "          ...,\n",
       "          [ 0.1929, -0.0445,  0.0991,  ..., -0.0953, -0.0030,  0.1984],\n",
       "          [ 0.2284, -0.0544,  0.0959,  ..., -0.0850, -0.0146,  0.2094],\n",
       "          [ 0.1768, -0.0626,  0.1009,  ..., -0.1069,  0.0117,  0.1584]]],\n",
       "        grad_fn=<ViewBackward0>),\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_layer = LlamaAttention(4, 32, True)\n",
    "x = torch.randn(2, 32, 32)\n",
    "\n",
    "attention_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1.],\n",
       "        [1., 1.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(2, 2).tril(diagonal=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from torch import nn\n",
    "\n",
    "class LlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        # 获得qkv向量\n",
    "        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 拼接kvcache\n",
    "        if past_key_value is not None:\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "\n",
    "        past_key_value = (key_states, value_states) if use_cache else None\n",
    "\n",
    "        # 计算attention权重\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        # 加入mask矩阵，decoder-only为下三角\n",
    "        if attention_mask is not None:\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "            dtype_min = torch.tensor(\n",
    "                torch.finfo(attn_weights.dtype).min, device=attn_weights.device, dtype=attn_weights.dtype\n",
    "            )\n",
    "            attn_weights = torch.max(attn_weights, dtype_min)\n",
    "\n",
    "        # 计算softmax，这里需要从fp16升为fp32\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2)\n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4028234663852886e+38\n",
      "-3.4028234663852886e+38\n",
      "1.1920928955078125e-07\n",
      "1.1754943508222875e-38\n",
      "1.1754943508222875e-38\n",
      "1e-06\n"
     ]
    }
   ],
   "source": [
    "from torch import finfo\n",
    "\n",
    "print(finfo(torch.float32).max)\n",
    "print(finfo(torch.float32).min)\n",
    "print(finfo(torch.float32).eps)\n",
    "print(finfo(torch.float32).tiny)\n",
    "print(finfo(torch.float32).smallest_normal)\n",
    "print(finfo(torch.float32).resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
